{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cloud Computing\n",
    "\n",
    "### 3. Get Access to GPU Instances \n",
    "\n",
    "**Elastic Computing** - the degree to which a system is able to adapt to workload changes by provisioning and de-provisioning resources in an autonomic manner, such that at each point in time the available resources match the current demand as closely as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical cross entropy:\n",
    "* looks at the ground truth label and the prediction, and outputs a large value if the prediction is far off from the ground truth, and a small value if the prediction is close to the ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import matplotlib.cm as cm \n",
    "# import numpy as np\n",
    "\n",
    "# fig = plt.figure(figsize=(20,20))\n",
    "# for i in range(6):\n",
    "#     ax = fig.add_subplot(1, 6, i+1, xticks=[], yticks=[])\n",
    "#     ax.imshow(X_train[i], cmap='gray')\n",
    "#     ax.set_title(str(Y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer Perceptron vs. Convolutional Neural Networks when dealing with images\n",
    "* MLPs take in vectors as input, they have no knowledge that the input vector was previously an image (aka an array) \n",
    "* CNNs make use of the fact that the input data is an image (a 2D matrix that can be represented with numbers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_input(img, ax):\n",
    "#     ax.imshow(img, cmap='gray')\n",
    "#     width, height = img.shape\n",
    "#     thresh = img.max()/2.5\n",
    "#     for x in range(width):\n",
    "#         for y in range(height):\n",
    "#             ax.annotate(str(round(img[x][y],2)), xy=(y,x),\n",
    "#                         horizontalalignment='center',\n",
    "#                         verticalalignment='center',\n",
    "#                         color='white' if img[x][y]<thresh else 'black')\n",
    "\n",
    "# fig = plt.figure(figsize = (12,12)) \n",
    "# ax = fig.add_subplot(111)\n",
    "# visualize_input(X_train[0], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale [0,255] --> [0,1]\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer-valued labels:\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "One-hot labels:\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# print first ten (integer-valued) training labels\n",
    "print('Integer-valued labels:')\n",
    "print(y_train[:10])\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# print first ten (one-hot) training labels\n",
    "print('One-hot labels:')\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 15.0300%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate test accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 17s 348us/step - loss: 0.0774 - acc: 0.9767 - val_loss: 0.0971 - val_acc: 0.9741\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 23s 478us/step - loss: 0.0639 - acc: 0.9807 - val_loss: 0.0836 - val_acc: 0.9786\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 18s 376us/step - loss: 0.0510 - acc: 0.9852 - val_loss: 0.1018 - val_acc: 0.9768\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 32s 677us/step - loss: 0.0437 - acc: 0.9869 - val_loss: 0.0907 - val_acc: 0.9778\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 31s 643us/step - loss: 0.0389 - acc: 0.9885 - val_loss: 0.0947 - val_acc: 0.9785\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 25s 519us/step - loss: 0.0338 - acc: 0.9900 - val_loss: 0.1010 - val_acc: 0.9781\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 20s 424us/step - loss: 0.0295 - acc: 0.9914 - val_loss: 0.1036 - val_acc: 0.9788\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 27s 552us/step - loss: 0.0283 - acc: 0.9920 - val_loss: 0.1034 - val_acc: 0.9808\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 23s 485us/step - loss: 0.0252 - acc: 0.9921 - val_loss: 0.1123 - val_acc: 0.9787\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 27s 556us/step - loss: 0.0241 - acc: 0.9931 - val_loss: 0.1140 - val_acc: 0.9798\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=10,\n",
    "          validation_split=0.2,\n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('mnist.model.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Connectivity \n",
    "\n",
    "MLPs\n",
    "* only use fully connected layers (lots of parameters for small 28x28 images)\n",
    "* only accepts vectors as input (no knowledge of where the pixels are located in reference to one another)\n",
    "\n",
    "CNNs\n",
    "* use sparsely connected layers \n",
    "* accepts matrices as input \n",
    "\n",
    "**locally connected layers** uses far fewer parameters than a densely connected layer \n",
    "* less prone to over fitting \n",
    "* better at teasing out patterns contained in image data\n",
    "* parameter sharing\n",
    "    \n",
    "weight sharing \n",
    "* common weights for different regions of an image \n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "CNN single layer \n",
    "* layer_i --> $\\text{CONV}(\\text{layer_i})$ --> $g(\\text{CONV}(\\text{layer_i}))$ --> output\n",
    "\n",
    "* Common to have 10s to 100s of filters/kernels for each CONV layer\n",
    "\n",
    "* edge detector filters are super importante for CNNs   \n",
    "* for 3D images (RGB), it's proper to have 3D filters \n",
    "\n",
    "* we're assuming filters are randomly generated \n",
    "    * randomly designed patterns that the filters are going to detect is the result of this... \n",
    "    * filters are updated at each epoch to minimize the (categorical cross entropy) loss function \n",
    "    \n",
    "### Striding and padding \n",
    "\n",
    "* striding: how many pixels/indices one slides over an image \n",
    "\n",
    "* padding: a border (of zeros) around the 2D matrix so as to maintain shape in the next layer  \n",
    "    * 'VALID' = no padding\n",
    "    * 'SAME' = shape of the convolved layer is the same as the input layer shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "Conv2D(filters, kernel_size, strides, padding, activation='relu', input_shape)\n",
    "\n",
    "# if Conv layer is right after input layer, you must specify input_shape=(height,width, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_35 (Conv2D)           (None, 100, 100, 16)      144       \n",
      "=================================================================\n",
      "Total params: 144\n",
      "Trainable params: 144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, strides=2, padding='valid', \n",
    "    activation='relu', input_shape=(200, 200, 2)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* output shape changes with the stride # and padding_type (and kernel_size if padding='VALID'\n",
    "* param # changes by the number of filters, kernel size, and input shape \n",
    "\n",
    "* in a convolutional layer:\n",
    "    * recall that there is one bias variable for each filter, thus num_biases = num_filters \n",
    "    * num_weights = num_filters $*$ filter_height $*$ filter_width $*$ depth of input_layer\n",
    "    * num_params = num_weights $+$ num_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality \n",
    "\n",
    "* The shape of a convolutional layer depends on the supplied values of kernel_size, input_shape, padding, and stride. Let's define a few variables:\n",
    "\n",
    "* K - the number of filters in the convolutional layer\n",
    "* F - the height and width of the convolutional filters\n",
    "* S - the stride of the convolution\n",
    "* H_in - the height of the previous layer\n",
    "* W_in - the width of the previous layer\n",
    "Notice that K = filters, F = kernel_size, and S = stride. Likewise, H_in and W_in are the first and second value of the input_shape tuple, respectively.\n",
    "\n",
    "The depth of the convolutional layer will always equal the number of filters K.\n",
    "\n",
    "If padding = 'same', then the spatial dimensions of the convolutional layer are the following:\n",
    "\n",
    "`height = ceil(float(H_in) / float(S))`\n",
    "\n",
    "`width = ceil(float(W_in) / float(S))`\n",
    "\n",
    "If padding = 'valid', then the spatial dimensions of the convolutional layer are the following:\n",
    "\n",
    "`height = ceil(float(H_in - F + 1) / float(S))`\n",
    "\n",
    "`width = ceil(float(W_in - F + 1) / float(S))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_49 (Conv2D)           (None, 2, 2, 32)          896       \n",
      "=================================================================\n",
      "Total params: 896\n",
      "Trainable params: 896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding='valid', \n",
    "    activation='relu', input_shape=(5, 5, 3)))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "* too high of a dimensionality of a convolutional layer leads to too many parameters, which can lead to overfitting, so pooling layers help shorten this \n",
    "    * expresses maxpooling and global average pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.pooling.MaxPooling2D at 0x18eaf74c1d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras MaxPooling example \n",
    "\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "pool_size, strides, padding = (1,1), 0, 'valid'\n",
    "\n",
    "MaxPooling2D(pool_size, strides, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling2d_16 (MaxPooling (None, 25, 25, 15)        0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(MaxPooling2D(pool_size=2, strides=4, input_shape=(100, 100, 15)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs for Image Classification \n",
    "\n",
    "* Conv layers detect regional patterns in an image\n",
    "* pooling layers reduce the dimensionality of our arrays \n",
    "* CNNs input images have to be all the same size (preprocessing must be done for this)\n",
    "\n",
    "While flowing through a CNN we,\n",
    "* go from, pixel by pixel representation, to, are there ears in this photo???? \n",
    "    * **spatial representation** lowers and **content representation** increases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation in Keras\n",
    "\n",
    "* invariant representation - image still has an object no matter the size/angle/location on the image the object is\n",
    "    * scale invariance \n",
    "    * rotation invariance \n",
    "    * translation invariance (embedded within CNNs)\n",
    "* **data augmentation** - adding training examples (object is rotated, shifted in the image) so that you can make stronger predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fun Fact\n",
    "ssh is a software package that enables secure system administration and file transfers over insecure networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Project: Image Augmentation\n",
    "* IF YOU HAVE TOO MANY KERNELS OPEN, YOU MIGHT BE USING TOO MUCH MEMORY! Remove all kernels from running so you can focus on one kernal on the AWS server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing CNNs \n",
    "\n",
    "* Visualizing activation and convolutional maps \n",
    "* taking filters from conv layers and constructing images that maximize their activations \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "* take learned understanding from one NN model and transfer it to another NN  \n",
    "    * for transfer learning: keep earlier layers and train on the new layers and finally a new output layer \n",
    "\n",
    "let good_dataset = the large, good, well-trained, critically acclaimed :P dataset\n",
    "\n",
    "* General process of tranfser learning:\n",
    "    * randomly initialize weights for new layers \n",
    "    * initialize rest of the weights with pre-trained weights (from the already learned, acclaimed gucci model, gg)\n",
    "    * re-train the whole NN \n",
    "    \n",
    "* if new dataset is small and similar to good_dataset:\n",
    "    * apply transfer learning to the end of the NN\n",
    "    \n",
    "* if new dataset is large and very different from good_dataset:\n",
    "    * apply transfer learning farther on in the NN \n",
    "    \n",
    "OR:\n",
    "\n",
    "    |                  | similar          | different        |\n",
    "    |------------------|------------------|------------------|\n",
    "    | small            | end of convNet   | start of convNet |\n",
    "    |------------------|------------------|------------------|\n",
    "    | large            | fine-tune        | fine-tune/retrain|\n",
    "    \n",
    "    \n",
    "    \n",
    "If the new data set is **small** and **similar** to the original training data:\n",
    "\n",
    "* slice off the end of the neural network\n",
    "* add a new fully connected layer that matches the number of classes in the new data set\n",
    "* randomize the weights of the new fully connected layer; **freeze** all the weights from the pre-trained network (to prevent overfitting)\n",
    "* train the network to update the weights of the new fully connected layer\n",
    "\n",
    "If the new data set is **small** and **different** from the original training data:\n",
    "\n",
    "* slice off most of the pre-trained layers near the beginning of the network\n",
    "* add to the remaining pre-trained layers a new fully connected layer that matches the number of classes in the new data set\n",
    "* randomize the weights of the new fully connected layer; **freeze** all the weights from the pre-trained network\n",
    "* train the network to update the weights of the new fully connected layer\n",
    "\n",
    "If the new data set is **large** and **similar** to the original training data:\n",
    "\n",
    "* remove the last fully connected layer and replace with a layer matching the number of classes in the new data set\n",
    "* randomly initialize the weights in the new fully connected layer\n",
    "* initialize the rest of the weights using the pre-trained weights\n",
    "* re-train the **entire** neural network\n",
    "\n",
    "If the new data set is **large** and **different** from the original training data:\n",
    "\n",
    "* remove the last fully connected layer and replace with a layer matching the number of classes in the new data set\n",
    "* retrain the network from scratch with randomly initialized weights for the new layers (and initialzing the pre-trained weights with their pre-trained weights) \n",
    "* alternatively, you could just use the same strategy as the \"large and similar\" data case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dog-Breed Classifier\n",
    "\n",
    "* **Cascade of Classifiers**\n",
    "    * instead of applying all features (6000 in the example) on a window,  the features are grouped into different stages of classifiers and applied one-by-one.\n",
    "        * if the window fails for a stage(?), drop it\n",
    "        * otherwise, if the window cascades through all the classifier stages, then that feature is detected in the image. (seems a little of an implementation for me, maybe my intuition is wrong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import numpy as np \n",
    "\n",
    "# load files returns a type Bunch, a dictionary-like object with 'interesting attributes'\n",
    "load_files('some_madeup_path')\n",
    "\n",
    "# stack arrays vertically \n",
    "np.vstack \n",
    "\n",
    "# Python Imaging Library\n",
    "from PIL import ImageFile\n",
    "\n",
    "# keras.preprocessing imports module image, which builds on top of PIL library \n",
    "from keras.preprocessing import image\n",
    "\n",
    "# loading a model with the best weights from checkpointers \n",
    "Xception_model.load_weights('saved_models/weights.best.VGG16.hdf5')\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n",
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0\n",
    "\n",
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
