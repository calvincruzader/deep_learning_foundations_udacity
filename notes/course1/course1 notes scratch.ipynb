{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy rundown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = np.array(5)\n",
    "print(s)\n",
    "print(s.shape)\n",
    "x = s + 3\n",
    "y = 8\n",
    "print(\"s type: %s\" % type(s))\n",
    "print(\"x=%d and is of type: %s\" % (x,type(x)))\n",
    "print(\"y=%d and is of type: %s\" % (y,type(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=np.array([1,2,3])\n",
    "print(\"v = %s and has shape = %s\" % (v, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuple : a finite, ordered sequence of elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_tuple=(123, 532, 'whaddup chaningas!') and is of type <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "my_tuple = 123, 532, \"whaddup chaningas!\"\n",
    "print(\"my_tuple=%s and is of type %s\" % (my_tuple,type(my_tuple)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuples are immutable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b4c0395966fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"something else\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "my_tuple[0] = \"something else\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v.shape)\n",
    "print(v.reshape(1,3).shape)\n",
    "print(v[None, :].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dot products only work for vectors of the same length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "v = [1,2,3]\n",
    "print(type(v))\n",
    "v_nd = np.ndarray(v)\n",
    "print(np.min(v_nd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(inputs):\n",
    "    # TODO: create a 2-dimensional ndarray from the given 1-dimensional list;\n",
    "    #       assign it to input_array\n",
    "    input_array = np.array(inputs)\n",
    "    \n",
    "    # TODO: find the minimum value in input_array and subtract that\n",
    "    #       value from all the elements of input_array. Store the\n",
    "    #       result in inputs_minus_min\n",
    "    inputs_minus_min = input_array - np.min(input_array)\n",
    "\n",
    "    # TODO: find the maximum value in inputs_minus_min and divide\n",
    "    #       all of the values in inputs_minus_min by the maximum value.\n",
    "    #       Store the results in inputs_div_max.\n",
    "    inputs_div_max = inputs_minus_min / np.max(inputs_minus_min)\n",
    "\n",
    "    # return the three arrays we've created\n",
    "    return input_array, inputs_minus_min, inputs_div_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "[[ 0.     0.375  1.   ]]\n"
     ]
    }
   ],
   "source": [
    "some_arr = [-1,2,7]\n",
    "a,b,c = prepare_inputs([some_arr])\n",
    "print(a.shape[0])\n",
    "print(b.ndim)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_tuple = 1,2,3\n",
    "np.ndarray(v_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_inputs(m1, m2):\n",
    "    # TODO: Check the shapes of the matrices m1 and m2. \n",
    "    #       m1 and m2 will be ndarray objects.\n",
    "    #\n",
    "    #       Return False if the shapes cannot be used for matrix\n",
    "    #       multiplication. You may not use a transpose\n",
    "    if m1.shape[1] != m2.shape[0] and m1.shape[0] != m2.shape[1]: return False\n",
    "\n",
    "    # TODO: If you have not returned False, then calculate the matrix product\n",
    "    #       of m1 and m2 and return it. Do not use a transpose,\n",
    "    #       but you swap their order if necessary\n",
    "    if m1.shape[1] == m2.shape[0]:\n",
    "        return np.matmul(m1,m2)\n",
    "    else:\n",
    "        return np.matmul(m2,m1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.array([[1,2],[1,2]])\n",
    "m2 = np.array([[1,2],[2,3],[444,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 2]]\n",
      "[[  1   2]\n",
      " [  2   3]\n",
      " [444   5]]\n",
      "[[  3   6]\n",
      " [  5  10]\n",
      " [449 898]]\n"
     ]
    }
   ],
   "source": [
    "print(m1)\n",
    "print(m2)\n",
    "print(multiply_inputs(m1,m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'avg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-ddfb6875343e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'avg'"
     ]
    }
   ],
   "source": [
    "np.avg(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    y_hat = np.dot(X,W) + b \n",
    "    m = len(y)\n",
    "    for index in range(m):\n",
    "        if y[index] == 0 and y_hat[index] >= 0:\n",
    "            W[0] = W[0] + learn_rate * X[index][0]\n",
    "            W[1] = W[1] + learn_rate * X[index][1]\n",
    "            b = b + learn_rate\n",
    "        if y[index] == 1 and y_hat[index] < 0:\n",
    "            W[0] = W[0] - learn_rate * X[index][0]\n",
    "            W[1] = W[1] - learn_rate * X[index][1]\n",
    "            b = b - learn_rate\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_linesnp.mean(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure that prediction outputs are not dependent on each other = one hot encoding \n",
    "\n",
    "maximum likelihood = give existing labels the highest probability of correct output   , and pick the best model \n",
    "\n",
    "cross entropy = the sum of (logarithms)*(-1) of the probabilities aka the negative log-likelihood\n",
    "   * the lower the cross-entropy, the better the predictions\n",
    "   \n",
    "\n",
    "the lower the probability of a single prediction = x yields a large -ln(x), so sum(-ln(X)) where X is a bunch of bad probabilities is a large value, so we want to minize the value of sum(-ln(X))\n",
    "\n",
    "\n",
    "better to minimize cross entropy than to maximize likelihood aka min(sum(-ln(X))) is better than product(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross entropy implementation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.asarray(Y)\n",
    "    P = np.asarray(P)\n",
    "    ce = -np.sum(np.multiply(Y, np.log(P)) + np.multiply((1-Y), np.log(1-P)))\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important concept: Measuring the quality of a NN\n",
    "#### Train of thought:\n",
    "We have a set of inputs, outputs, and a model, so the question is: how do we tune the parameters in our model so that we can predict the outputs of futures inputs?\n",
    "\n",
    "A measure of error $E$ will show us **how well** the model predictions fare on the outputs across an entire dataset.\n",
    "\n",
    "The first thought is to do $E = \\sum\\limits_{\\mu}(y - \\hat{y})$, but we choose $E = \\dfrac{1}{2}\\sum\\limits_{\\mu}(y - \\hat{y})^2$ (SSE) for a couple of reasons:\n",
    "* the error is always positive\n",
    "* the square penalizes outliers (bad predictions) more \n",
    "* it'll help with our weight updates later on (i.e. $w_i = w_i + \\Delta w_i)$\n",
    "* the $\\dfrac{1}{2}$ makes the derivative easier to work with later on\n",
    "\n",
    "Now, we strive to create the **highest** quality of a neural network that can predict the outputs with the highest possible accuracy (100%) so we'd want to minimize the error, or $\\min E$. And, since the parameters we'll be tuning are the weights $W$ (remember in forward prop, $W$ are all the weights we use to calculate our prediction $\\hat{y}$ when given input data. Also, this is not including hyperparameters: initalization, activation functions, etc.), we have: $\\min\\limits_{W} E$.\n",
    "\n",
    "So, what is  $\\min\\limits_{W} E = ?$\n",
    "\n",
    "Sometimes it can be solved analytically, but MOSTLY it can't! So, we'll need to solve for it iteratively, where the error is smaller with each successive iteration we reach a sufficient condition/threshold, through optimization algorithms. Most popular optim algo in use right now is gradient descent.\n",
    "\n",
    "Gradient descent is as follows:\n",
    "\n",
    "Remember that $\\hat{y} = f(\\sum w_ix_i) \\rightarrow E = \\dfrac{1}{2}\\sum\\limits_{\\mu}(y - f(\\sum w_ix_i))^2$. Let's say our error for varying values of $W$ can be represented as such: \n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/6/6d/Error_surface_of_a_linear_neuron_with_two_input_weights.png)\n",
    "\n",
    "We'd want to reach the bottom of this bowl so that we __minimize__ the error. Since $f(\\sum w_ix_i)= f(W,X)$ and $X$ is a predetermined set of inputs, we'll have to tune $W$ to move ourselves along the graph. Let's call a single descent step of this movement $\\Delta W$ or, equivalently, $\\Delta w, \\forall w \\in W$.\n",
    "\n",
    "Now, it turns out that this single descent step $\\Delta W = -\\nabla E(W,X)$, or the negative of the gradient of $E$. \n",
    "\n",
    "---\n",
    "Validation that a single descent step $\\Delta W = -\\nabla E(W,X)$, slight side tangent (HA). \n",
    "\n",
    "So, if you remember from calc1 that $f'(x)$ is some instantaneous rate of change for some function $f$, or, more verbosely, the rate of a nudge in the function when x is nudged, or $\\dfrac{df(x)}{dx}$. Let's assume $f$ is a quadratic, nonnegative polynomial. Loosely, $\\dfrac{df(x)}{dx}$ will give us the rate at which we're moving away from the minimum of the parabola (do some slopes and plot some points, this will make sense when you draw it out). To move closer to the minimum, we have to take $-\\dfrac{df(x)}{dx}$. (We'll also have to take some learning rate $\\alpha < 1$ so we don't diverge (plot some more points for clarity). \n",
    "\n",
    "\n",
    "The way we develop a function for our error $E$ is similar. $E$ will always be non-negative (we can't have $y - \\hat{y} < 0$, because $\\hat{y}$ is some probability with a range of $[0,1]$ and $y$ is some discrete value $ y \\in \\{0,1\\}$) and we'll strive to create $E$ such that it's analagous to the bowl example above because it won't be a 3D bowl, we'll most likely deal with $E \\in \\mathbb{R} ^ n, n > 2$, a hyperdimensional bowl I guess. We'll want to move closer to the minimum of this hyperdimensional bowl so we'll need to take the negative of the rate at which $E$ is nudged when a single $w_i$ is nudged. Thus, we have $-\\dfrac{\\partial E}{\\partial w_i}, \\forall w_i \\in W = -\\nabla E(W,X)$. \n",
    "\n",
    "---\n",
    "\n",
    "So, now we'll want to **iteratively update** $W$ such that we are closer to to $\\min\\limits_{W} E$ with each step $i$, $W_{i+1} := W_i + \\Delta W_i$, where $E(W_{i+1},X) < E(W_i,X)$. However, as we saw in our 1dimensional example, we can diverge if we take too big of a nudge, so we'll have to scale down the step by some learning rate $\\alpha$, so the actual equation for an update step can be written as: $$W_{i+1} := W_i + \\alpha \\Delta W_i$$\n",
    "\n",
    "Recall that:\n",
    "* $W$ can be represented as an $n$-dimensional vector containing all the weights in a NN to parameterize.\n",
    "* $-\\nabla E$ is an $n$-dimensional vector containing the partial derivatives of each weight, i.e.: \n",
    "$$ -\\nabla E = -\\left[ \\begin{array}{ccc} \\dfrac{\\partial E}{\\partial w_1} \\\\ \\dfrac{\\partial E}{\\partial w_2} \\\\ \\vdots \\\\ \\dfrac{\\partial E}{\\partial w_n} \\end{array} \\right]$$\n",
    "\n",
    "Thus, we can represent $W + \\Delta W$ as: \n",
    "\n",
    "$$W + \\alpha \\Delta W = W -\\alpha \\nabla E = \\left[ \\begin{array}{ccc} w_1 - \\alpha\\dfrac{\\partial E}{\\partial w_1} \\\\ w_2 - \\alpha\\dfrac{\\partial E}{\\partial w_2} \\\\ \\vdots \\\\ w_n - \\alpha\\dfrac{\\partial E}{\\partial w_n} \\end{array} \\right]$$\n",
    "\n",
    "So, this begs the question: what is $\\dfrac{\\partial E}{\\partial w_i} \\forall i \\in \\{1,...,n\\}$?\n",
    "\n",
    "Recall that for 1 training example, $E = \\dfrac{1}{2}(y - \\hat{y})^2$, so let's do some math:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\dfrac{\\partial E}{\\partial w_i} &= \\dfrac{\\partial}{\\partial w_i} \\dfrac{1}{2}(y-\\hat{y})^2 \\\\ &= (y - \\hat{y})\\dfrac{\\partial}{\\partial w_i}(y - \\hat{y}) \\because \\text{ chain rule} \\\\ &= (y - \\hat{y})(0 -\\dfrac{\\partial}{\\partial w_i} \\hat{y}) \\\\ &= - (y - \\hat{y})\\dfrac{\\partial}{\\partial w_i} \\hat{y} \\\\ &= -(y-\\hat{y})\\dfrac{\\partial}{\\partial w_i} f(h), \\text{ where } h = \\sum\\limits_{i=1}^n w_i x_i\\\\ &= -(y-\\hat{y})f'(h)\\dfrac{\\partial}{\\partial w_i} \\sum\\limits_{i=1}^n w_i x_i \\because \\text{ chain rule} \\\\ &= -(y-\\hat{y})f'(h)x_i\n",
    "\\end{align}$$\n",
    "\n",
    "Thus, to perform a single update step, we'd have $\\forall w_i \\in W$:\n",
    "$$\\begin{align}\n",
    " w_i + \\alpha\\Delta w_i &= w_i + \\alpha(- \\dfrac{\\partial E}{\\partial w_i}) \\\\ &= w_i - \\alpha(-(y-\\hat{y})f'(h)x_i) \\\\ &= w_i + \\alpha(y-\\hat{y})f'(h)x_i \\\\ &= w_i + \\alpha \\delta x_i\n",
    "\\end{align}$$ \n",
    "\n",
    "where we define $\\delta = (y-\\hat{y})f'(h)$ as the 'error term'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "\n",
    "Through backpropagation, we feed the **error** backwords through the neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a neural network with 1 hidden layer, the error $\\delta$ for one hidden unit $j$ is:\n",
    "\n",
    "$$\\Large\\delta^{h}_{j} = \\sum{W_{jk}\\delta^{o}_{k}f'(h_{j})}$$\n",
    "\n",
    "$\\delta^{h}_{j}$ is a unit $j$ in the hidden layer $h$\n",
    "\n",
    "$W_{jk}$ is the weights between the hidden unit $j$ and the $k$th output unit \n",
    "\n",
    "$\\delta^{o}_{k}$ is the error of the $k$th output unit \n",
    "\n",
    "$f'(h_{j})$ is the derivative of the activation function \n",
    "\n",
    "So, basically, the sum of the output errors scaled by the weights that connect the hidden unit $\\delta_j^h$ is the total error for $\\delta_j^h$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
