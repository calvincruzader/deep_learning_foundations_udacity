{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks \n",
    "\n",
    "## RNN Intro \n",
    "\n",
    "Recurrent is defined as occuring often or repeatedly\n",
    "\n",
    "For RNNs, we perform the same task for each element in the input sequence \n",
    "\n",
    "## RNN History \n",
    "\n",
    "First attempt to add memory to neural networks were Time Delay Neural Networks (TDNNs 1989)\n",
    "\n",
    "Simple RNN/ Elman Networks (1990) followed \n",
    "\n",
    "Vanishing gradient problem was a problem for RNNs too\n",
    "    contributions of information decayed geometrically over time \n",
    "    \n",
    "Long Short-Term Memory (LSTMs mid-90s) addressed the vanishing gradient problem \n",
    "* some signals (state variables) are kept fixed by using gates and reintroduced (or not) at an appropraite time in the future \n",
    "* arbitrary time intervals can be represented and temporal dependnecies can be captured  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Applications\n",
    "\n",
    "* speech recognition, time series prediction, gesture recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Prop and BackProp\n",
    "\n",
    "Has a bunch of useful calculations that I derived earlier, easier to read imo than the initial time we went through it in course1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Why are they called RNNs?\n",
    "* Perform the same task for each element in the input sequence\n",
    "\n",
    "RNNs have **memory elements** or **states** that attempt to retain previous information of previous inputs \n",
    "\n",
    "Temporal dependencies \n",
    "* the current output depends on both a current input and also memory element which takes into account past inputs \n",
    "\n",
    "Two fundamental differences between RNNs and FFNNs\n",
    "* We only considered the current input for FFNNs, but with RNNs we consider previous inputs/outputs through representing the inputs and outputs in a sequence \n",
    "* RNNs have memory elements (hidden neuron in FFNNs), which we also consider previous iterations of through representing these memory elements in a sequence \n",
    "\n",
    "RNN **memory** is defined as the output of the hidden layer which serves as additional input to the network at the following training step  \n",
    "* $h$ notation (hidden) is changed to $\\bar{s}_t$ for state \n",
    "\n",
    "RNNs can be \"unfolded in time,\" what we use when working with RNNs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - Unfolded Model \n",
    "\n",
    "* looks cleaner \n",
    "\n",
    "# Backpropagation Through Time (BPTT)\n",
    "\n",
    "* train network at timestep $T$ as well as take into account all the previous time steps \n",
    "\n",
    "**Accumulative Gradient**:\n",
    "* to update the the matrix $W_s$ at some time step $N$, we need $\\dfrac{\\partial E_N}{\\partial W_s}$ such that:\n",
    "    * $\\dfrac{\\partial E_N}{\\partial W_s} = \\sum\\limits^N_{i=1} \\dfrac{\\partial E_N}{\\partial \\bar{y}_N}\\dfrac{\\partial \\bar{y}_N}{\\partial \\bar{s}_i} \\dfrac{\\partial \\bar{s}_i}{\\partial W_s}$\n",
    "    * This will take into account each individual time step's gradient, accumulating it as we go through time.\n",
    "\n",
    "* similarly, for $W_x$, at some time step $N$, we have $\\dfrac{\\partial E_N}{\\partial W_x}$ such that:\n",
    "    * $\\dfrac{\\partial E_N}{\\partial W_s} = \\sum\\limits^N_{i=1} \\dfrac{\\partial E_N}{\\partial \\bar{y}_N}\\dfrac{\\partial \\bar{y}_N}{\\partial \\bar{s}_i} \\dfrac{\\partial \\bar{s}_i}{\\partial W_x}$\n",
    "    \n",
    "* we consider **all** paths when accumulating the gradient.\n",
    "    * i have a big issue with this if you want to build deep NNs, it just seems infeasibly expensive....\n",
    "    * oh ok, apparently **LSTM**s address this issue \n",
    "\n",
    "# RNN Summary\n",
    "\n",
    "* **gradient clipping** addresses the exploding gradient problem\n",
    "https://arxiv.org/abs/1211.5063 \n",
    "\n",
    "# From RNN to LSTM \n",
    "\n",
    "A couple reasons why (actuallyyy, they're fundamental problems lol):\n",
    "* avoid the loss of information\n",
    "* avoid the vanishing gradient problem \n",
    "\n",
    "* **paper**!\n",
    "http://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
