{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks \n",
    "\n",
    "## RNN Intro \n",
    "\n",
    "Recurrent is defined as occuring often or repeatedly\n",
    "\n",
    "For RNNs, we perform the same task for each element in the input sequence \n",
    "\n",
    "## RNN History \n",
    "\n",
    "First attempt to add memory to neural networks were Time Delay Neural Networks (TDNNs 1989)\n",
    "\n",
    "Simple RNN/ Elman Networks (1990) followed \n",
    "\n",
    "Vanishing gradient problem was a problem for RNNs too\n",
    "    contributions of information decayed geometrically over time \n",
    "    \n",
    "Long Short-Term Memory (LSTMs mid-90s) addressed the vanishing gradient problem \n",
    "* some signals (state variables) are kept fixed by using gates and reintroduced (or not) at an appropraite time in the future \n",
    "* arbitrary time intervals can be represented and temporal dependnecies can be captured  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Applications\n",
    "\n",
    "* speech recognition, time series prediction, gesture recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Prop and BackProp\n",
    "\n",
    "Has a bunch of useful calculations that I derived earlier, easier to read imo than the initial time we went through it in course1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Why are they called RNNs?\n",
    "* Perform the same task for each element in the input sequence\n",
    "\n",
    "RNNs have **memory elements** or **states** that attempt to retain previous information of previous inputs \n",
    "\n",
    "Temporal dependencies \n",
    "* the current output depends on both a current input and also memory element which takes into account past inputs \n",
    "\n",
    "Two fundamental differences between RNNs and FFNNs\n",
    "* We only considered the current input for FFNNs, but with RNNs we consider previous inputs/outputs through representing the inputs and outputs in a sequence \n",
    "* RNNs have memory elements (hidden neuron in FFNNs), which we also consider previous iterations of through representing these memory elements in a sequence \n",
    "\n",
    "RNN **memory** is defined as the output of the hidden layer which serves as additional input to the network at the following training step  \n",
    "* $h$ notation (hidden) is changed to $\\bar{s}_t$ for state \n",
    "\n",
    "RNNs can be \"unfolded in time,\" what we use when working with RNNs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - Unfolded Model \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
