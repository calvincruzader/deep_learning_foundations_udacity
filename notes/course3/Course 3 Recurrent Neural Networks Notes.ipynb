{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks \n",
    "\n",
    "## RNN Intro \n",
    "\n",
    "Recurrent is defined as occuring often or repeatedly\n",
    "\n",
    "For RNNs, we perform the same task for each element in the input sequence \n",
    "\n",
    "## RNN History \n",
    "\n",
    "First attempt to add memory to neural networks were Time Delay Neural Networks (TDNNs 1989)\n",
    "\n",
    "Simple RNN/ Elman Networks (1990) followed \n",
    "\n",
    "Vanishing gradient problem was a problem for RNNs too\n",
    "    contributions of information decayed geometrically over time \n",
    "    \n",
    "Long Short-Term Memory (LSTMs mid-90s) addressed the vanishing gradient problem \n",
    "* some signals (state variables) are kept fixed by using gates and reintroduced (or not) at an appropraite time in the future \n",
    "* arbitrary time intervals can be represented and temporal dependnecies can be captured  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Applications\n",
    "\n",
    "* speech recognition, time series prediction, gesture recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Prop and BackProp\n",
    "\n",
    "Has a bunch of useful calculations that I derived earlier, easier to read imo than the initial time we went through it in course1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Why are they called RNNs?\n",
    "* Perform the same task for each element in the input sequence\n",
    "\n",
    "RNNs have **memory elements** or **states** that attempt to retain previous information of previous inputs \n",
    "\n",
    "Temporal dependencies \n",
    "* the current output depends on both a current input and also memory element which takes into account past inputs \n",
    "\n",
    "Two fundamental differences between RNNs and FFNNs\n",
    "* We only considered the current input for FFNNs, but with RNNs we consider previous inputs/outputs through representing the inputs and outputs in a sequence \n",
    "* RNNs have memory elements (hidden neuron in FFNNs), which we also consider previous iterations of through representing these memory elements in a sequence \n",
    "\n",
    "RNN **memory** is defined as the output of the hidden layer which serves as additional input to the network at the following training step  \n",
    "* $h$ notation (hidden) is changed to $\\bar{s}_t$ for state \n",
    "\n",
    "RNNs can be \"unfolded in time,\" what we use when working with RNNs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - Unfolded Model \n",
    "\n",
    "* looks cleaner \n",
    "\n",
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "* train network at timestep $T$ as well as take into account all the previous time steps \n",
    "\n",
    "**Accumulative Gradient**:\n",
    "* to update the the matrix $W_s$ at some time step $N$, we need $\\dfrac{\\partial E_N}{\\partial W_s}$ such that:\n",
    "    * $\\dfrac{\\partial E_N}{\\partial W_s} = \\sum\\limits^N_{i=1} \\dfrac{\\partial E_N}{\\partial \\bar{y}_N}\\dfrac{\\partial \\bar{y}_N}{\\partial \\bar{s}_i} \\dfrac{\\partial \\bar{s}_i}{\\partial W_s}$\n",
    "    * This will take into account each individual time step's gradient, accumulating it as we go through time.\n",
    "\n",
    "* similarly, for $W_x$, at some time step $N$, we have $\\dfrac{\\partial E_N}{\\partial W_x}$ such that:\n",
    "    * $\\dfrac{\\partial E_N}{\\partial W_s} = \\sum\\limits^N_{i=1} \\dfrac{\\partial E_N}{\\partial \\bar{y}_N}\\dfrac{\\partial \\bar{y}_N}{\\partial \\bar{s}_i} \\dfrac{\\partial \\bar{s}_i}{\\partial W_x}$\n",
    "    \n",
    "* we consider **all** paths when accumulating the gradient.\n",
    "    * i have a big issue with this if you want to build deep NNs, it just seems infeasibly expensive....\n",
    "    * oh ok, apparently **LSTM**s address this issue \n",
    "\n",
    "## RNN Summary\n",
    "\n",
    "* **gradient clipping** addresses the exploding gradient problem\n",
    "https://arxiv.org/abs/1211.5063 \n",
    "\n",
    "## From RNN to LSTM \n",
    "\n",
    "A couple reasons why (actuallyyy, they're fundamental problems lol):\n",
    "* avoid the loss of information\n",
    "* avoid the vanishing gradient problem \n",
    "\n",
    "* **paper**!\n",
    "http://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Networks (LSTM)\n",
    "\n",
    "Some pre-course reading warm-ups \n",
    "* http://blog.echen.me/2017/05/30/exploring-lstms/\n",
    "* https://www.youtube.com/watch?v=iX5V1WpxxkY\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "## RNN vs LSTM\n",
    "\n",
    "* Due to the vanishing gradient problem, RNNs have a hard time getting information from iterations of the distant past (in the example, the probability of recognizing a wolf is relating to bears recognized in the distant past)\n",
    "\n",
    "LSTMs keep track of both **long term memory** and **short term memory** oh my god.\n",
    "\n",
    "## Basics of LSTM \n",
    "\n",
    "Every LSTM cell has 4 gates: \n",
    "* forget gate \n",
    "* remember gate \n",
    "* learn gate \n",
    "* use gate \n",
    "\n",
    "## Architecture of LSTM \n",
    "\n",
    "\n",
    "## Learn Gate \n",
    "\n",
    "**Combines** short term memory and the new event, **ignores** some of them, and passes them to the output\n",
    "\n",
    "Let $\\text{STM}_{t-1}$ be the short term memory at time $t-1$ and $E_t$ the event at time $t$, the information it actually passes is represented by: \n",
    "\n",
    "* Combine step:\n",
    "$N_t = \\tanh(W_n[\\text{STM}_{t-1}, E_t] + b_n)$\n",
    "\n",
    "* Ignore step: apply $i_t$\n",
    "where $i_t$ is the 'ignore factor', a vector, but we multiply element-wise. We calculate $i_t$ by passing $\\text{STM}_{t-1},$ and  $E_t$ through as small neural network such that:\n",
    "\n",
    "$i_t = \\sigma(W_i[\\text{STM}_{t-1}, E_t] + b_i)$\n",
    "  \n",
    "Thus we have that the learn gate is $N_t \\times i_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forget Gate \n",
    "\n",
    "Chooses which information to keep and which to forget. We multiply long term memory from the last iteration $\\text{LTM}_{t-1}$ by a forget factor $f_t$ such that:\n",
    "\n",
    "$f_t = \\sigma(W_f[\\text{STM}_{T-1}, E_t] + b_f)$\n",
    "\n",
    "Then, we have $\\text{LTM}_{t-1} \\times f_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember Gate \n",
    "\n",
    "Add the Forget Gate and the Learn Gate outputs such that: \n",
    "\n",
    "$\\text{LTM}_{t-1} \\times f_t + N_t \\times i_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Gate \n",
    "\n",
    "also known as the output gate, uses the output of the Forget Gate and the short term memory of the previous iteration and the event, we have:\n",
    "\n",
    "$U_t = \\tanh(W_u\\text{LTM}_{t-1} \\times f_t + b_u)$\n",
    "\n",
    "$V_t = \\sigma(W_v[\\text{STM}_{t-1}, E_t] + b_v)$\n",
    "\n",
    "Output is (a new short term memory output which is equivalent to the output of the cell):\n",
    "\n",
    "$\\text{STM}_t = U_t * V_t$\n",
    "\n",
    "## Resources! DL4J\n",
    "\n",
    "https://deeplearning4j.org/lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of RNN and LSTM \n",
    "\n",
    "## Sequence batching\n",
    "\n",
    "batch up the input to efficiently use matrix operations \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
