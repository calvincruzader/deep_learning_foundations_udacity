{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks \n",
    "\n",
    "## RNN Intro \n",
    "\n",
    "Recurrent is defined as occuring often or repeatedly\n",
    "\n",
    "For RNNs, we perform the same task for each element in the input sequence \n",
    "\n",
    "## RNN History \n",
    "\n",
    "First attempt to add memory to neural networks were Time Delay Neural Networks (TDNNs 1989)\n",
    "\n",
    "Simple RNN/ Elman Networks (1990) followed \n",
    "\n",
    "Vanishing gradient problem was a problem for RNNs too\n",
    "    contributions of information decayed geometrically over time \n",
    "    \n",
    "Long Short-Term Memory (LSTMs mid-90s) addressed the vanishing gradient problem \n",
    "* some signals (state variables) are kept fixed by using gates and reintroduced (or not) at an appropraite time in the future \n",
    "* arbitrary time intervals can be represented and temporal dependnecies can be captured  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Applications\n",
    "\n",
    "* speech recognition, time series prediction, gesture recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Prop and BackProp\n",
    "\n",
    "Has a bunch of useful calculations that I derived earlier, easier to read imo than the initial time we went through it in course1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Why are they called RNNs?\n",
    "* Perform the same task for each element in the input sequence\n",
    "\n",
    "RNNs have **memory elements** or **states** that attempt to retain previous information of previous inputs \n",
    "\n",
    "Temporal dependencies \n",
    "* the current output depends on both a current input and also memory element which takes into account past inputs \n",
    "\n",
    "Two fundamental differences between RNNs and FFNNs\n",
    "* We only considered the current input for FFNNs, but with RNNs we consider previous inputs/outputs through representing the inputs and outputs in a sequence \n",
    "* RNNs have memory elements (hidden neuron in FFNNs), which we also consider previous iterations of through representing these memory elements in a sequence \n",
    "\n",
    "RNN **memory** is defined as the output of the hidden layer which serves as additional input to the network at the following training step  \n",
    "* $h$ notation (hidden) is changed to $\\bar{s}_t$ for state \n",
    "\n",
    "RNNs can be \"unfolded in time,\" what we use when working with RNNs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - Unfolded Model \n",
    "\n",
    "* looks cleaner \n",
    "\n",
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "* train network at timestep $T$ as well as take into account all the previous time steps \n",
    "\n",
    "**Accumulative Gradient**:\n",
    "* to update the the matrix $W_s$ at some time step $N$, we need $\\dfrac{\\partial E_N}{\\partial W_s}$ such that:\n",
    "    * $\\dfrac{\\partial E_N}{\\partial W_s} = \\sum\\limits^N_{i=1} \\dfrac{\\partial E_N}{\\partial \\bar{y}_N}\\dfrac{\\partial \\bar{y}_N}{\\partial \\bar{s}_i} \\dfrac{\\partial \\bar{s}_i}{\\partial W_s}$\n",
    "    * This will take into account each individual time step's gradient, accumulating it as we go through time.\n",
    "\n",
    "* similarly, for $W_x$, at some time step $N$, we have $\\dfrac{\\partial E_N}{\\partial W_x}$ such that:\n",
    "    * $\\dfrac{\\partial E_N}{\\partial W_s} = \\sum\\limits^N_{i=1} \\dfrac{\\partial E_N}{\\partial \\bar{y}_N}\\dfrac{\\partial \\bar{y}_N}{\\partial \\bar{s}_i} \\dfrac{\\partial \\bar{s}_i}{\\partial W_x}$\n",
    "    \n",
    "* we consider **all** paths when accumulating the gradient.\n",
    "    * i have a big issue with this if you want to build deep NNs, it just seems infeasibly expensive....\n",
    "    * oh ok, apparently **LSTM**s address this issue \n",
    "\n",
    "## RNN Summary\n",
    "\n",
    "* **gradient clipping** addresses the exploding gradient problem\n",
    "https://arxiv.org/abs/1211.5063 \n",
    "\n",
    "## From RNN to LSTM \n",
    "\n",
    "A couple reasons why (actuallyyy, they're fundamental problems lol):\n",
    "* avoid the loss of information\n",
    "* avoid the vanishing gradient problem \n",
    "\n",
    "* **paper**!\n",
    "http://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Networks (LSTM)\n",
    "\n",
    "Some pre-course reading warm-ups \n",
    "* http://blog.echen.me/2017/05/30/exploring-lstms/\n",
    "* https://www.youtube.com/watch?v=iX5V1WpxxkY\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "## RNN vs LSTM\n",
    "\n",
    "* Due to the vanishing gradient problem, RNNs have a hard time getting information from iterations of the distant past (in the example, the probability of recognizing a wolf is relating to bears recognized in the distant past)\n",
    "\n",
    "LSTMs keep track of both **long term memory** and **short term memory** oh my god.\n",
    "\n",
    "## Basics of LSTM \n",
    "\n",
    "Every LSTM cell has 4 gates: \n",
    "* forget gate \n",
    "* remember gate \n",
    "* learn gate \n",
    "* use gate \n",
    "\n",
    "## Architecture of LSTM \n",
    "\n",
    "\n",
    "## Learn Gate \n",
    "\n",
    "**Combines** short term memory and the new event, **ignores** some of them, and passes them to the output\n",
    "\n",
    "Let $\\text{STM}_{t-1}$ be the short term memory at time $t-1$ and $E_t$ the event at time $t$, the information it actually passes is represented by: \n",
    "\n",
    "* Combine step:\n",
    "$N_t = \\tanh(W_n[\\text{STM}_{t-1}, E_t] + b_n)$\n",
    "\n",
    "* Ignore step: apply $i_t$\n",
    "where $i_t$ is the 'ignore factor', a vector, but we multiply element-wise. We calculate $i_t$ by passing $\\text{STM}_{t-1},$ and  $E_t$ through as small neural network such that:\n",
    "\n",
    "$i_t = \\sigma(W_i[\\text{STM}_{t-1}, E_t] + b_i)$\n",
    "  \n",
    "Thus we have that the learn gate is $\\text{Learn_Gate} = N_t \\times i_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forget Gate \n",
    "\n",
    "Chooses which information to keep and which to forget. We multiply long term memory from the last iteration $\\text{LTM}_{t-1}$ by a forget factor $f_t$ such that:\n",
    "\n",
    "$f_t = \\sigma(W_f[\\text{STM}_{T-1}, E_t] + b_f)$\n",
    "\n",
    "Then, we have $\\text{Forget_Gate }=\\text{LTM}_{t-1} \\times f_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember Gate \n",
    "\n",
    "Add the Forget Gate and the Learn Gate outputs such that: \n",
    "\n",
    "$\\text{Remember Gate} = \\text{LTM}_{t-1} \\times f_t + N_t \\times i_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Gate \n",
    "\n",
    "also known as the output gate, uses the output of the Forget Gate and the short term memory of the previous iteration and the event, we have:\n",
    "\n",
    "$U_t = \\tanh(W_u\\text{LTM}_{t-1} \\times f_t + b_u)$\n",
    "\n",
    "$V_t = \\sigma(W_v[\\text{STM}_{t-1}, E_t] + b_v)$\n",
    "\n",
    "Output is (a new short term memory output which is equivalent to the output of the cell):\n",
    "\n",
    "$\\text{STM}_t = U_t * V_t$\n",
    "\n",
    "## Resources! DL4J\n",
    "\n",
    "https://deeplearning4j.org/lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of RNN and LSTM \n",
    "\n",
    "## Sequence batching\n",
    "\n",
    "batch up the input to efficiently use matrix operations \n",
    "\n",
    "## Implementing a Character-wise RNN \n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generator functions: defined like a regular function, but have the keyword `yield` that returns a value (or values) \n",
    "## and freezes the state of the function until called again (via .next() or the next iteration in a for loop)\n",
    "\n",
    "# Generator functions are useful in that they keep memory pool small while computing iterative processes.\n",
    "\n",
    "def get_up_to_some(num):\n",
    "    counter = 0\n",
    "    while counter < num:\n",
    "        yield counter \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "hello! after the for loop\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Example of calling a generator function. Notice how the state is frozen even after going out of the for loop. \n",
    "\n",
    "calvs_generator = get_up_to_some(10)\n",
    "\n",
    "for i in calvs_generator:\n",
    "    print(i)\n",
    "    if i == 5: \n",
    "        break\n",
    "print(\"hello! after the for loop\")\n",
    "print(next(calvs_generator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "arr = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "n_seqs = 2 \n",
    "n_steps = 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "characters_per_batch = n_seqs * n_steps \n",
    "print(characters_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(arr) // characters_per_batch\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "arr = arr[:n_batches * characters_per_batch]\n",
    "print(arr)\n",
    "# keep only enough to keep complete batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "arr = arr.reshape((n_seqs, -1))\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    def build_cell(keep_prob):\n",
    "        \n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "        # Add dropout to the cell outputs\n",
    "        # DropoutWrapper accepts an rnn cell and has different probabilities to keep for output, input, and state cells \n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "        \n",
    "    # stack up multiple LSTM layers, all with a given lstm_size \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    # initialize all the lstm layers as zeros \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t1 = [[1, 2, 3], [4, 5, 6]]\n",
    "t2 = [[7, 8, 9], [10, 11, 12]]\n",
    "dim0 = tf.concat([t1, t2], 0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "dim1 = tf.concat([t1, t2], 1)  # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]\n",
    "\n",
    "# t3 = [2,3]\n",
    "# t4 = [2,3]\n",
    "\n",
    "# tf.shape(tf.concat([t3, t4], 0))  # [4, 3]\n",
    "# tf.shape(tf.concat([t3, t4], 1))  # [2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat_5:0\", shape=(2, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(dim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/\n",
    "\n",
    "## Prog assignment, building LSTMs \n",
    "\n",
    "so hard to understand this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "Once we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n",
    "\n",
    "> **Exercise:** Implement the output layer in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x,softmax_w) + b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters \n",
    "\n",
    "## Learning Rate \n",
    "\n",
    "Learning Rate Decay\n",
    "* linear learning rate decay\n",
    "* exponential learning rate decay\n",
    "* adaptive learning rate \n",
    "    * adjust learning rate based what's happening with the error:\n",
    "        * if error is decreasing too slowly, increase learning rate \n",
    "        * if error 'bounces around'/diverges (for example), decrease learning rate \n",
    "        \n",
    "Tensorflow readings:\n",
    "\n",
    "Adagrad: https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer\n",
    "\n",
    "Adam: https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "\n",
    "Exponential decay: https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Size  \n",
    "\n",
    "* too large minibatch size (1024), could be too computationally taxing and worse accuracy potentially\n",
    "\n",
    "## Number of Iterations \n",
    "\n",
    "* In choosing the number of iterations/ the epochs, we should look at the validation error\n",
    "    * if validation error goes up, then apply early stopping to the model at the point of lowest validation error \n",
    "    \n",
    "TF validation monitors: https://www.tensorflow.org/get_started/#early_stopping_with_validationmonitor\n",
    "\n",
    "Validation monitors have went away in favor of SessionRunHooks:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook\n",
    "\n",
    "Varying types of hooks:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hidden units \n",
    "\n",
    "You'd generally want more hidden units for more complex functions \n",
    "\n",
    "Too many hidden units can cause overfitting \n",
    "\n",
    "Keep adding hidden units until the validation error starts to get worse \n",
    "\n",
    "Having the first hidden layer larger in number than the input layer has been seen to be beneficial in a number of tasks \n",
    "\n",
    "Deeper Conv nets perform better, otherwise FFNNs in some cases seem to work really well with 3 layers, but YMMV with more layers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN hyperparameters \n",
    "\n",
    "Choosing a cell type: \n",
    "* LSTM \n",
    "* vanilla RNN cell \n",
    "* gated recurrent unit cell (GRU)   \n",
    "    * 1 and 3 work perform better in practice \n",
    "How deep the layer is, how many hidden layers do we stack\n",
    "\n",
    "Embedding dimensionality \n",
    "* more than 500 dimensions is rare \n",
    "\n",
    "LSTM vs GRU \n",
    "* depends on the task \n",
    "\n",
    "LSTM is really good for **advanced speech recognition** \n",
    "\n",
    "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n",
    "\n",
    "https://arxiv.org/abs/1412.3555\n",
    "\n",
    "An Empirical Exploration of Recurrent Network Architectures\n",
    "\n",
    "http://proceedings.mlr.press/v37/jozefowicz15.pdf\n",
    "\n",
    "Visualizing and Understanding Recurrent Networks\n",
    "\n",
    "https://arxiv.org/abs/1506.02078\n",
    "\n",
    "LSTM: A Search Space Odyssey\n",
    "\n",
    "https://arxiv.org/pdf/1503.04069.pdf\n",
    "\n",
    "Understanding LSTM Networks\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Massive Exploration of Neural Machine Translation Architectures\n",
    "\n",
    "https://arxiv.org/abs/1703.03906v2\n",
    "\n",
    "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition\n",
    "\n",
    "https://arxiv.org/abs/1610.09975\n",
    "\n",
    "**Speech Recognition with Deep Recurrent Neural Networks**\n",
    "\n",
    "https://arxiv.org/abs/1303.5778\n",
    "\n",
    "Sequence to Sequence Learning with Neural Networks\n",
    "\n",
    "https://arxiv.org/abs/1409.3215\n",
    "\n",
    "Show and Tell: A Neural Image Caption Generator\n",
    "\n",
    "https://arxiv.org/abs/1411.4555\n",
    "\n",
    "DRAW: A Recurrent Neural Network For Image Generation\n",
    "\n",
    "https://arxiv.org/abs/1502.04623\n",
    "\n",
    "A Long Short-Term Memory Model for Answer Sentence Selection in\n",
    "Question Answering\n",
    "\n",
    "http://www.aclweb.org/anthology/P15-2116\n",
    "\n",
    "SEQUENCE-TO-SEQUENCE RNNS FOR\n",
    "TEXT SUMMARIZATION\n",
    "\n",
    "https://pdfs.semanticscholar.org/3fbc/45152f20403266b02c4c2adab26fb367522d.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources \n",
    "\n",
    "Practical recommendations for gradient-based training of deep architectures\n",
    "\n",
    "https://arxiv.org/abs/1206.5533\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters\n",
    "\n",
    "Efficient Backprop\n",
    "\n",
    "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "\n",
    "How to Generate a Good Word Embedding?\n",
    "\n",
    "https://arxiv.org/abs/1507.05523\n",
    "\n",
    "Systematic evaluation of CNN advances on the ImageNet\n",
    "\n",
    "https://arxiv.org/abs/1606.02228\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Word2vec \n",
    "\n",
    "## Embeddings Intro \n",
    "\n",
    "Word embeddings are NN method for representing data with a huge number of classes more efficiently.\n",
    "* greatly improve the abilitiy of networks to learn data by representing data with lower dimensional vectors.\n",
    "* Given training data, finds relationships between words that are usually correlated together ('United', 'States') vs. ('United', 'Pancakes')\n",
    "\n",
    "## Implementing Word2vec\n",
    "\n",
    "Readings\n",
    "\n",
    "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of word2vec from Chris McCormick \n",
    "    * If two words have similar 'contexts' then the model should output similar word vectors for these two\n",
    "\n",
    "    * But the neural network is too big! (300 x 10,000  x num_training_data), we'll need to enhance this by:\n",
    "        * treating word pairs or 'phrases' as their own 'word' (ie word vector representation) in the model. So 'Boston' and 'Globe' have a really different meaning separately than if used together, 'Boston Globe', so 'Boston Globe' has it's own word vector representation in the model.\n",
    "        * subsampling is used as well. There's a chance a word is deleted from the text based on frequency.\n",
    "        * negative sampling is used as well. Only a subset of the weights are updated, with the negative samples being more likely to be updated \n",
    "\n",
    "* [First word2vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "* [NIPS paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for word2vec also from Mikolov et al.\n",
    "* An [implementation of word2vec](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/) from Thushan Ganegedara\n",
    "* TensorFlow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)\n",
    "\n",
    "Continuous Bag of Words (CBOW) : context around your word\n",
    "* 'The quick brown fox jumped over the lazy dog', if you're looking at fox and the context around fox, then CBOW would be something like ('quick', 'brown', 'jumped' ,'over'), and shift this CBOW continually as you move along the sentence.\n",
    "\n",
    "Skip-gram\n",
    "* You have a word and we try to predict what context that word appears in \n",
    "\n",
    "Efficiency of list comprehensions: \n",
    "\n",
    "http://blog.cdleary.com/2010/04/efficiency-of-list-comprehensions/\n",
    "\n",
    "# AUTO ML???\n",
    "\n",
    "http://automl.info/\n",
    "\n",
    "http://www.ml4aad.org/automl/\n",
    "\n",
    "https://futurism.com/google-artificial-intelligence-built-ai/\n",
    "\n",
    "https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
